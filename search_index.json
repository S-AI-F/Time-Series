[
["index.html", "Time Series with R Chapter 1 Introduction", " Time Series with R Saif Shabou 2020-12-10 Chapter 1 Introduction This tutorial presents a synthesis of various methods and tools used for analyzing time series data with R: Time series decomposition Time series modeling Time series forecasting Outlier detection in time series data "],
["time-series-basics.html", "Chapter 2 Time series basics 2.1 What is a Time Series 2.2 Time Series exploration 2.3 Time Series patterns 2.4 Stationarity in Time Series 2.5 Auto-correlation 2.6 References", " Chapter 2 Time series basics 2.1 What is a Time Series 2.1.1 Definition Any metric that is measured over regular time intervals makes a Time Series (examples: weather data, stock prices, census analysis, budgetary analysis…). We can define a time series as a series of data points indexed in time order. It is a sequence taken at successive equally spaecd points in time, a sequence of discrete-time data. The purpose of time series analysis is generally twofold: to understand or model the stochastic mechanisms that gives rise to an observed series and to predict or forecast the future values of a series based on the history of that series 2.2 Time Series exploration We will implement some basic operations on Time serie data. We can use AirPassengers preloaded dataset. data(AirPassengers) # show the class of data class(AirPassengers) ## [1] &quot;ts&quot; # print data print(AirPassengers) ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 Time series summary: We can use simple commands to show first/last dates and records frequency: # show the start/end dates of the time serie data start(AirPassengers) ## [1] 1949 1 end(AirPassengers) ## [1] 1960 12 # show the data frequency frequency(AirPassengers) ## [1] 12 # show summary of air passengers value summary(AirPassengers) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 104.0 180.0 265.5 280.3 360.5 622.0 Trend analysis: We can easily plot the time serie data and fit a linear model #This will plot the time series ts.plot(AirPassengers, xlab=&quot;Year&quot;, ylab=&quot;Number of Passengers&quot;, main=&quot;Monthly totals of international airline passengers, 1949-1960&quot;) # This will fit in a line abline(reg=lm(AirPassengers~time(AirPassengers)), col = &quot;red&quot;) Cycles: The command cycle gives the positions in the cycle of each observation cycle(AirPassengers) ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 1 2 3 4 5 6 7 8 9 10 11 12 ## 1950 1 2 3 4 5 6 7 8 9 10 11 12 ## 1951 1 2 3 4 5 6 7 8 9 10 11 12 ## 1952 1 2 3 4 5 6 7 8 9 10 11 12 ## 1953 1 2 3 4 5 6 7 8 9 10 11 12 ## 1954 1 2 3 4 5 6 7 8 9 10 11 12 ## 1955 1 2 3 4 5 6 7 8 9 10 11 12 ## 1956 1 2 3 4 5 6 7 8 9 10 11 12 ## 1957 1 2 3 4 5 6 7 8 9 10 11 12 ## 1958 1 2 3 4 5 6 7 8 9 10 11 12 ## 1959 1 2 3 4 5 6 7 8 9 10 11 12 ## 1960 1 2 3 4 5 6 7 8 9 10 11 12 Aggregation: We can aggregate the cycles ad display a year on year trend plot(aggregate(AirPassengers,FUN=mean)) Box plot across months will give us a sense on seasonal effect boxplot(AirPassengers~cycle(AirPassengers)) Components: Each data point \\(Y_t\\) at time \\(t\\) in a Time Series can be expressed as either a sum or a product of 3 components, namely, Seasonality (\\(S_t\\)), Trend (\\(T_t\\)) and Error (\\(e_t\\)) (a.k.a White Noise). Additive Time Series: \\(Y_t = S_t + T_t + e_t\\) Multiplicative Time Series: \\(Y_t = S_t * T_t * e_t\\) 2.3 Time Series patterns Trend: A trend exists when there is a long-term increase or decrease in the data. Seasonal: A seasonal apptern occurs when a tie series is affected by seasonal factors such as the day of the week or the time of the daya. Seosonaity refers to a known frequency. Cyclic A cycle occurs when the data shows rises and falls that are not of a fixed frequency. 2.3.1 How to extract the trend, seasonality and error? The decompose() and forecast::stl() splits the time series into seasonality, trend and error components. tsData &lt;- EuStockMarkets[, 1] # ts data decomposedRes &lt;- decompose(tsData, type=&quot;mult&quot;) # use type = &quot;additive&quot; for additive components plot (decomposedRes) # see plot below stlRes &lt;- stl(tsData, s.window = &quot;periodic&quot;) 2.3.2 How to de-trend a time series ? Use linear regression to model the Time Series data with linear indices (Ex: 1, 2, .. n). The resulting model’s residuals is a representation of the time series devoid of the trend. trModel &lt;- lm(JohnsonJohnson ~ c(1:length(JohnsonJohnson))) plot(resid(trModel), type=&quot;l&quot;) # resid(trModel) contains the de-trended series. 2.3.3 How to de-seasonalize a time series in R? De-seasonalizing throws insight about the seasonal pattern in the time series and helps to model the data without the seasonal effects. In order to de-seasonalize a time series we need to de-compose first the time series using forecast::stl() , and then use seasadj() from forecast package library(forecast) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo ts.stl &lt;- stl(tsData ,&quot;periodic&quot;) # decompose the TS ts.sa &lt;- seasadj(ts.stl) # de-seasonalize plot(AirPassengers, type=&quot;l&quot;) # original series plot(ts.sa, type=&quot;l&quot;) # seasonal adjusted seasonplot(ts.sa, 12, col=rainbow(12), year.labels=TRUE, main=&quot;Seasonal plot: Airpassengers&quot;) # seasonal frequency set as 12 for monthly data. 2.4 Stationarity in Time Series 2.4.1 Elementary statistics Before defining auto-correlation in Time Series analysis, we will introduce some basic concepts necessary to understand statical modeling of time series objects. Expectation: The expectation (\\(E(x)\\)) of a random variable \\(x\\) is its mean average value in the population. We denote the expectation of \\(x\\) by \\(\\mu\\) such that \\(E(x) = \\mu\\) Variance: The variance of a random variable is the expectation of the squared deviations of the variable from the mean, denoted by \\(\\sigma^2(x) = E[(x-\\mu)^2]\\) Standard Deviation: The standard deviation of a random variable \\(x\\), \\(\\sigma(x)\\), is the square root of the variance of x. Covariance: Covariance tells us how lineary related are two random variables. Given two random variables \\(x\\) and \\(y\\) with repsective expectations \\(\\mu_x\\) and \\(\\mu_y\\), the covariance is \\(\\sigma(x,y) = E[(x-\\mu_x)(y-\\mu_y)]\\). In statstical situation, we should estimate the covariance from a sample of population. Therefore we use the sample means \\(\\overline{x}\\) and \\(\\overline{y}\\). The sample covariance is calculated as follow: \\[Cov(x,y) = \\frac{1}{n-1} \\sum_{i = 1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})\\] Let’s generate two random variables and calculate their covariance: set.seed(1) x &lt;- seq(1,100) + 20.0*rnorm(1:100) set.seed(2) y &lt;- seq(1,100) + 20.0*rnorm(1:100) plot(x,y) cov(x,y) ## [1] 681.6859 Correlation: As opposite to covariance, correlation is a dimensionless measure of how two variabes vary together. It consists of “covariance” of two random variables normalized by their respective spreads. \\[Cor(x,y) = \\frac{Cov(x,y)}{\\sigma(x)\\sigma(y)}\\] When \\(Cor(x,y) = 1\\), it means exact positive linear association. When \\(Cor(x,y) = 1\\), it shows no linear association at all and when \\(Cor(x,y) = -1\\), it indicates exact negative linear asssociation. Let’s calculate the correlation of the previous vectors \\(x\\) and \\(y\\): cor(x,y) ## [1] 0.5796604 2.4.2 Stationarity definition There are three main critera for considering a series as stationary: The mean of the series should not be a function of time but should be a constant. We consider a time series as stationary in the mean if \\(\\mu(t) = \\mu\\), a constant. The variance of the series should not be a function of time. This property is called: homoscedasticity. Hence, a time series is stationary in the variance if \\(\\sigma^2(t) =\\sigma^2\\), a constant. The covariance of \\(x(i)\\) and \\(x(i+m)\\) should not be a function of time. Second order stationary: A time series is second order stationary if the correlation between sequential observations is only a function of the lag (the number of time steps seperating each sequential observation). There are various techniques to bring stationarity to a non-stationay time series: Detrending: Removing the trend component from the time series by using \\(log()\\) Diffrencing: Modeling the differences of the terms instead of actual terms 2.4.3 Stationarity test In order to test the stationarity of a time series, we can use the Augmented Dickey-Fuller Test (adf test). A p-Value of less than 0.05 in adf.test() indicates that the time series is stationary. library(tseries) tsData &lt;- EuStockMarkets[, 1] # ts data adf.test(tsData) # p-value &lt; 0.05 indicates the TS is stationary ## ## Augmented Dickey-Fuller Test ## ## data: tsData ## Dickey-Fuller = -0.82073, Lag order = 12, p-value = 0.9598 ## alternative hypothesis: stationary kpss.test(tsData) ## Warning in kpss.test(tsData): p-value smaller than printed p-value ## ## KPSS Test for Level Stationarity ## ## data: tsData ## KPSS Level = 15.401, Truncation lag parameter = 8, p-value = 0.01 2.4.4 Make a time series stationary Differencing: Differencing a time series means, to subtract each data point in the series from its successor. It is commonly used to make a time series stationary. For most time series patterns, 1 or 2 differencing is necessary to make it a stationary series. But if the time series appears to be seasonal, a better approach is to difference with respective season’s data points to remove seasonal effect. After that, if needed, difference it again with successive data points. But, How to know how many differencing is needed? The nsdiffs and ndiffs from forecast package can help find out how many seasonal differencing and regular differencing respectively is needed to make the series stationary. library(tseries) library(forecast) # Seasonal Differencing nsdiffs(AirPassengers) # number for seasonal differencing needed ## [1] 1 #&gt; 1 AirPassengers_seasdiff &lt;- diff(AirPassengers, lag=frequency(AirPassengers), differences=1) # seasonal differencing plot(AirPassengers_seasdiff, type=&quot;l&quot;, main=&quot;Seasonally Differenced&quot;) # still not stationary! # Make it stationary ndiffs(AirPassengers_seasdiff) # number of differences need to make it stationary ## [1] 1 #&gt; 1 stationaryTS &lt;- diff(AirPassengers_seasdiff, differences= 1) plot(stationaryTS, type=&quot;l&quot;, main=&quot;Differenced and Stationary&quot;) 2.5 Auto-correlation Autocorrelations or lagged correlations are used to know whether a time series is dependant on its past (how sequential observations in a time series affect each other). For a time series \\(x\\) of length \\(n\\) we consider the \\(n-1\\) pairs of observations one time unit apart. The \\(lag-1\\) autocorrelation of \\(x\\) can be estimated as the sample correlation of these \\([x_{t},x_{t-1}]\\) pairs. Auto-covariance of a Time Series: If a time series is second order stationary then the autocovariance, of lag k, \\(C_k = E[(x_{t} - \\mu)(x_{t+k} - \\mu)]\\). The autocovariance \\(C_k\\) is not a function of time. \\[C_k = \\frac{1}{n} \\sum_{t = 1}^{n-k} (x_t - \\overline{x})(x_{t+k} - \\overline{x})\\] Auto-correlation of a Time Series: The auto-correlation of lag \\(k\\) of a second order stationaru time series is given by the autocovariance of the series normalized by the product od spread. \\[r_k = \\frac{C_k}{C_0}\\] The correolgram: A correlogramn is a plot of the autocorrelation function for sequential values of lag \\(k = 0,1,...,n\\). It shows the correlation structure in each lag. We can use the command acf to plot a correlogram. Here are some examples of correlograms fir simulated time series: w &lt;- seq(1, 100) w ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 acf(w) We notice that the ACF plot decreases in an almost linear way as the lags increase.A correlogram of this type is clear indication of a trend. w &lt;- rep(1:10, 10) w ## [1] 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 ## [26] 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 ## [51] 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 ## [76] 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 acf(w) We can see that at lag 10 and 20 there are significant peaks because sequneces are repeating with a period of 10. We see that there is a negative correlation at lags 5 and 15 of -0,5. It is characteristics of sesonal time series. 2.6 References https://blogs.rstudio.com/ai/posts/2018-06-25-sunspots-lstm/ http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/ https://www.tensorflow.org/tutorials/structured_data/time_series https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/ https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/ https://www.quantstart.com/articles/Serial-Correlation-in-Time-Series-Analysis/ "],
["arma-time-series-modeling.html", "Chapter 3 ARMA Time Series modeling 3.1 Auto-Regressive Time Series model 3.2 Moving Average Time Series Model 3.3 Model selection: AR or MA 3.4 References", " Chapter 3 ARMA Time Series modeling 3.1 Auto-Regressive Time Series model The Auto-Regressive (AR) model can be interpreted as a simple linear regression where each observation \\(x(t)\\) is regressed on the previous observation \\(x(t-1)\\). It can be formulated with the following equation known as AR(1) formulation: \\[x(t) = \\alpha \\times x(t-1) + error(t)\\] The equation denotes that the current instance is dependant on the previous one with an alpha coefficient which we seek to minimize the error function. If \\(\\alpha = 0\\) then \\(x(t)\\) is simply white noise. Large \\(\\alpha\\) show a high correlation and negative values of \\(\\alpha\\) result in oscillatory time series. Fitting an AR model: We can use the command arima() in order to fit an AR model to a time series. An AR model is an ARIMA model with order (1,0,0). #Fitting the AR Model to the time series AR &lt;- arima(AirPassengers, order = c(1,0,0)) print(AR) ## ## Call: ## arima(x = AirPassengers, order = c(1, 0, 0)) ## ## Coefficients: ## ar1 intercept ## 0.9646 278.4649 ## s.e. 0.0214 67.1141 ## ## sigma^2 estimated as 1119: log likelihood = -711.09, aic = 1428.18 #plotting the series along with the fitted values ts.plot(AirPassengers) AR_fit &lt;- AirPassengers - residuals(AR) points(AR_fit, type = &quot;l&quot;, col = 2, lty = 2) Forecasting using an AR model: We can make forecasts using the estimated AR model using the command predict which gives us the predicted value ($pred) and standard error of the forecast ($se). #Using predict() to make a 1-step forecast predict_AR &lt;- predict(AR) predict_AR ## $pred ## Jan ## 1961 426.5698 ## ## $se ## Jan ## 1961 33.44577 #Obtaining the 1-step forecast using $pred[1] predict_AR$pred[1] ## [1] 426.5698 # predicting 10 step forecats predict(AR, n.ahead = 10) ## $pred ## Jan Feb Mar Apr May Jun Jul Aug ## 1961 426.5698 421.3316 416.2787 411.4045 406.7027 402.1672 397.7921 393.5717 ## Sep Oct ## 1961 389.5006 385.5735 ## ## $se ## Jan Feb Mar Apr May Jun Jul Aug ## 1961 33.44577 46.47055 55.92922 63.47710 69.77093 75.15550 79.84042 83.96535 ## Sep Oct ## 1961 87.62943 90.90636 We can visualize the forecasts with 95% prediction intervals ts.plot(AirPassengers, xlim = c(1949, 1961)) AR_forecast &lt;- predict(AR, n.ahead = 10)$pred AR_forecast_se &lt;- predict(AR, n.ahead = 10)$se points(AR_forecast, type = &quot;l&quot;, col = 2) points(AR_forecast - 2*AR_forecast_se, type = &quot;l&quot;, col = 2, lty = 2) points(AR_forecast + 2*AR_forecast_se, type = &quot;l&quot;, col = 2, lty = 2) 3.2 Moving Average Time Series Model The Moving Average (MA) model has a regression llike form, but each observation is regressed on the previous innovation, which is not actually observed. A weighted sum of previous and current noise is called Moving Average (MA) model. It can be interpreted as: each observation \\(x(t)\\) is regressed on previous noise \\(error(t-1)\\). It can be formulated with the following equation known as MA(1) formulation: \\[x(t) = \\beta \\times error(t-1) + error(t)\\] The \\(\\beta\\) is a coefficient which we seek to mimize the error function. Fitting an AR model: We can use the command arima() in order to fit a MA model to a time series. A MA model is an ARIMA model with order (0,0,1). #Fitting the MA model to AirPassengers MA &lt;- arima(AirPassengers, order = c(0,0,1)) print(MA) ## ## Call: ## arima(x = AirPassengers, order = c(0, 0, 1)) ## ## Coefficients: ## ma1 intercept ## 0.9642 280.6464 ## s.e. 0.0214 10.5788 ## ## sigma^2 estimated as 4205: log likelihood = -806.43, aic = 1618.86 #plotting the series along with the MA fitted values ts.plot(AirPassengers) MA_fit &lt;- AirPassengers - resid(MA) points(MA_fit, type = &quot;l&quot;, col = 2, lty = 2) Forecasting using an AR model: We can make forecasts using the estimated AR model using the command predict which gives us the predicted value ($pred) and standard error of the forecast ($se). # Making a 1-step through 10-step forecast based on MA predict(MA,n.ahead=10) ## $pred ## Jan Feb Mar Apr May Jun Jul Aug ## 1961 425.1049 280.6464 280.6464 280.6464 280.6464 280.6464 280.6464 280.6464 ## Sep Oct ## 1961 280.6464 280.6464 ## ## $se ## Jan Feb Mar Apr May Jun Jul Aug ## 1961 64.84895 90.08403 90.08403 90.08403 90.08403 90.08403 90.08403 90.08403 ## Sep Oct ## 1961 90.08403 90.08403 We can visualize the forecasts with 95% prediction intervals #Plotting the AIrPAssenger series plus the forecast and 95% prediction intervals ts.plot(AirPassengers, xlim = c(1949, 1961)) MA_forecasts &lt;- predict(MA, n.ahead = 10)$pred MA_forecast_se &lt;- predict(MA, n.ahead = 10)$se points(MA_forecasts, type = &quot;l&quot;, col = 2) points(MA_forecasts - 2*MA_forecast_se, type = &quot;l&quot;, col = 2, lty = 2) points(MA_forecasts + 2*MA_forecast_se, type = &quot;l&quot;, col = 2, lty = 2) 3.3 Model selection: AR or MA We can you godness of fit in order to choose the more appropriate model. Two metrics are commonly used in time series model: Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). A lower value indicates a better fitting model. These two indicators penalize models with more estimated parameters in order to avoid overfitting. Akaike Information Criterion (AIC): Bayesian Information Criterion (BIC): # Compute AIC of AR model AIC(AR) ## [1] 1428.179 # Compute AIC of MA model AIC(MA) ## [1] 1618.863 # Compute BIC of AR model BIC(AR) ## [1] 1437.089 # Compute BIC of MA model BIC(MA) ## [1] 1627.772 Based on the results, we can select the AR model since it gives lower value of AIC and BIC. 3.4 References https://rpubs.com/JSHAH/481706 "],
["time-series-forecasting.html", "Chapter 4 Time Series Forecasting 4.1 Understanding the context of forecasting 4.2 Data preparation 4.3 Case study 4.4 Performance metrics 4.5 Naive methods 4.6 Exponential smoothing 4.7 ARIMA/SARIMA models", " Chapter 4 Time Series Forecasting Making predictions about the future is called extrapolation in the classical statistical handling of time series data. More modern fields focus on the topic and refer to it as time series forecasting. Forecasting involves taking models fit on historical data and using them to predict future observations. 4.1 Understanding the context of forecasting Before implementing a forecasting model, it is essential to understand the goal in order to fix some directions and constraints: Data availability: Knowing how much data is available is a first step. More data is often more helpful, offering greater opportunity for exploratory data analysis, model testing and tuning, and model fidelity. Time horizon: Identifying the time horizon of predictions that is required (short, medium or long term). Shorter time horizons are often easier to predict with higher confidence. Frequency of forecasts update: Updating forecasts as new information becomes available often results in more accurate predictions. -Temporal frequency of forecasts: Often forecasts can be made at a lower or higher frequencies, allowing you to harness down-sampling, and up-sampling of data. 4.2 Data preparation Time series data often requires various transformation before applying a forecasting model. For example: -Frequency: Perhaps data is provided at a frequency that is too high to model or is unevenly spaced through time requiring resampling for use in some models. Outliers: Perhaps there are corrupt or extreme outlier values that need to be identified and handled. Missing data: Perhaps there are gaps or missing data that need to be interpolated or imputed. 4.3 Case study We will use the Forecast R package and ‘AirPassengers’ dataset for this section. library(forecast) library(MLmetrics) ## ## Attaching package: &#39;MLmetrics&#39; ## The following object is masked from &#39;package:base&#39;: ## ## Recall data=AirPassengers #Create samples training=window(data, start = c(1949,1), end = c(1955,12)) validation=window(data, start = c(1956,1)) 4.4 Performance metrics Before presenting forecasting models, this section describes various model performance metrics often used for time series applications. 4.4.1 Forecast errors A forecast “error” is the difference between an observed value and its forecast. Here “error” does not mean a mistake, it means the unpredictable part of an observation. 4.4.2 R-squared \\[R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\\] where \\(SS_{res}\\) is the sum of squared residuals from the predicted values and \\(SS_{tot}\\) is the sum of squared deviations of the dependent variable from the sample mean. It means how much of the variance in the dependent variable can explain the variance in the independent variable. High value means the variance in the model is similar to the variance in the true values and if the R2 value is low it means that the two values are not much correlated. The important point to note in the case of R-squared is that, it does not show if the model is satisfactory future predictions or not. It shows if the model is a good fit observed values and how good of a “fit” it is. High \\(R^2\\) means that the correlation between observed and predicted values is high. 4.4.3 Mean Absolute Error (MSE) Mean absolute error is the average of the absolute values of the deviation. This type of error measurement is useful when measuring prediction errors in the same unit as the original series. See the below formula \\[MAE = \\frac{\\sum_{i = 0}^{n} |y_i - \\hat{y_i}|}{n}\\] Thus MAE will tell you how big of an error can you expect from the forecast on average. 4.4.4 Median Absolute Error (MedAE) Median absolute error(MedAE) is similar to the MAE. To calculate MedAE, take the absolute differences and then find the median value. \\[MedAE = median(|y_1 - \\hat{y_1}|,|y_2 - \\hat{y_2}|,...,|y_n - \\hat{y_n}|)\\] Using median is an extreme way of triming extreme values. Hence median absolute error reduces the bias in favor of low forecasts. 4.4.5 Mean Squared Error (MSE) The mean squared error is the average of the square of the forecast error. As the square of the errors are taken, the effect is that larger errors have more weight on the score. \\[MSE = \\frac{\\sum_{i = 0}^{n} (y_i - \\hat{y_i})^2}{n}\\] Since both MSE and RMSE takes the square of the errors, outliers will have a huge effect on the resulting error. 4.4.5.1 Mean Absolute Percentage Error (MAPE) As this is a percentage error and hence gives a good idea of the relative error. Percentage errors have the advantage of being unit-free, and so are frequently used to compare forecast performances between data sets. \\[MAPE = \\frac{100%}{n}\\sum_{i = 1}^{n} \\frac{y_i - \\hat{y_i}}{y_i}\\] 4.5 Naive methods Any forecasting method should be evaluated by being compared to a naive method. This helps ensuring that the efforts put in having a more complex model are worth in terms of performance: Simple naive: the forecast for tomorrow is what we are observing today. Seasonal naive: the forecast of tomorrow is what we observed the week/month/year (depending what horizon we are working with) before. naive = snaive(training, h=length(validation)) MAPE(naive$mean, validation) * 100 ## [1] 27.04689 That gves us a MAPE of 27.04%. That is the score to beat. plot(data, col=&quot;blue&quot;, xlab=&quot;Year&quot;, ylab=&quot;Passengers&quot;, main=&quot;Seasonal Naive Forecast&quot;, type=&#39;l&#39;) lines(naive$mean, col=&quot;red&quot;, lwd=2) 4.6 Exponential smoothing Forecast produced using exponential smoothing methods are weighted average of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the obserbation the higher the associated weight. 4.6.1 State Space Models With the forecast Package, smoothing methods can be placed within the structure of state space models. By using this structure, we can find the optimal exponential smoothing model, using the ets function. ets_model = ets(training, allow.multiplicative.trend = TRUE) summary(ets_model) ## ETS(M,Md,M) ## ## Call: ## ets(y = training, allow.multiplicative.trend = TRUE) ## ## Smoothing parameters: ## alpha = 0.6749 ## beta = 0.0082 ## gamma = 1e-04 ## phi = 0.98 ## ## Initial states: ## l = 119.9101 ## b = 1.0177 ## s = 0.9065 0.7973 0.9184 1.0524 1.1863 1.1987 ## 1.0896 0.9798 0.9957 1.046 0.9167 0.9126 ## ## sigma: 0.0395 ## ## AIC AICc BIC ## 726.4475 736.9706 770.2022 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 0.9845273 7.241276 5.726611 0.2712157 2.907857 0.2145244 0.0453492 We see ETS (M, Md, M). This means we have an ets model with multiplicative errors, a multiplicative trend and a multiplicative seasonality. Basically, mutliplicative means that the parameter is “amplified” over time. Here is how to forecast using the estimated optimal smoothing model: ets_forecast = forecast(ets_model, h=length(validation)) # plot plot(data, col=&quot;blue&quot;, xlab=&quot;Year&quot;, ylab=&quot;Passengers&quot;, main=&quot;Seasonal Naive Forecast&quot;, type=&#39;l&#39;) lines(ets_forecast$mean, col=&quot;red&quot;, lwd=2) # error MAPE(ets_forecast$mean, validation) *100 ## [1] 12.59147 We see that the upward trend in demand is being capture a little bit (far from perfect, better than naive). It gives an MAPE of 12.6%. 4.6.2 Double seasonal Holt-Winters The ets function only allows for one seasonality. Sometimes, the data we have can be composed of multiple seasonalities (monthly and yearly for instance). Double Seasonal Holt-Winters (DSHW) allows for two seasonalities: a smaller one repeated often and a bigger one repeated less often. For the method to work however, the seasonalities need to be nested, meaning one must be an integer multiple of the other (2 and 4, 24 and 168, etc.). The code here is a bit different since we need to specify the lenghts of our two seasonalities (which is not always something we know) and the forecast is computed directly when creating the model with the dshw function. dshw_model = dshw(training, period1=4, period2 = 12, h=length(validation)) MAPE(dshw_model$mean, validation)*100 ## [1] 3.27219 # plot plot(data, col=&quot;blue&quot;, xlab=&quot;Year&quot;, ylab=&quot;Passengers&quot;, main=&quot;Seasonal Naive Forecast&quot;, type=&#39;l&#39;) lines(dshw_model$mean, col=&quot;red&quot;, lwd=2) We get a MAPE of 3.7% with this method! 4.7 ARIMA/SARIMA models ARIMA models contain 3 components: AR (p): Autoregressive part of the model. Means that we use p past observations from the timeseries as predictors Differenciating (d): Used to transform the timeseries into a strationary one by taking the differences between successive observations at appropriate lags (d). MA (q): uses q past forecast errors as predicators That’s it for ARIMA but if you know the data you have is seasonal, then you need more. That’s where SARIMA comes into play. SARIMA adds a seasonal part to the model. The auto.arima function can be used to return the best estimated model. Here is the code: arima_optimal = auto.arima(training) To forecast a SARIMA model (which is what we have here since we have a seasonal part), we can use the sarima.for function from the astsa package. library(astsa) ## ## Attaching package: &#39;astsa&#39; ## The following object is masked from &#39;package:forecast&#39;: ## ## gas sarima_forecast = sarima.for(training, n.ahead=length(validation), p=0,d=1,q=1,P=1,D=1,Q=0,S=12) MAPE(sarima_forecast$pred, validation) * 100 ## [1] 6.544624 We get a MAPE of 6.5% with this SARIMA model. "],
["outlier-detection-in-time-series.html", "Chapter 5 Outlier detection in Time series 5.1 Introduction 5.2 Statistical-based approaches 5.3 Forecasting-based approaches 5.4 Neural Network Based Approaches 5.5 Clustering Based Approaches 5.6 Proximity Based Approaches 5.7 Tree Based Approaches 5.8 Dimension Reduction Based Approaches 5.9 References", " Chapter 5 Outlier detection in Time series 5.1 Introduction 5.1.1 Definition An outlier ia a value or an observation that is distant from other observations, a data point that differ significantly from other data points. A widely used deinition for the concept of outier has been provided by Hawkins: “An observation which deviates so much from other observations as to arouse suspicions that it wasgenerated by a different mechanism.”. It sometimes makes sense to fromally distinguish two classes of outliers: Extreme values and mistakes. Virtually all outlier detection algorithms create a model of the normal patterns in the data, and then compute an outlier score of a given data point on the basis of the deviations from these patterns. The selected model make different assumptions about the “normal” behavior of the data. The outlier score of a data point is then computed by evaluating the quality of the fit between the data points and the model. In practice, the choice of the model is often dictated by the analyst‘s understanding of the kinds of deviations relevant to an application. 5.1.2 Taxonomy Outlier detection techniques in time series data vary depending on the input data, the outlier type , and the nature of the method. 5.1.2.1 Input data: Univariate vs Multivariate Outlier detection methods may differ depending on the charcteristics of time series data: Univariate time series VS Mutivariate time series. A univariate detection method only considers a single time-dependent variable, whereas a multivariate detection method is able to simultaneously work with more than one time-dependent variable 5.1.2.2 Outlier type Outlier detection methods may differ depending on the type pf ouliers: Point outlier: A point outlier is a datum that behaves unusually in a specific time instant when compared either to the other values in the time series (global outlier) or to its neighboring points (local outlier). Subsequences: This term refers to consecutive points in time whose joint behavior is unusual, although each observation individually is not necessarily a point outlier Time Series: Entire time series can also be outliers, but they can only be detected when the input data is a multivariate time series. 5.1.2.3 Detection method Model-based: The most popular and intuitive definition for the concept of point outlier is a point that significantly deviates from its expected value. Therefore, given a univariate time series, a point at time t can be declared an outlier if the distance to its expected value is higher than a predefined threshold. If \\(\\hat{x}_t\\) is obtained using previous and subsequent observations to \\(x_t\\) (past, current, and future data), then the technique is within the estimation model-based methods. In contrast, if \\(\\hat{x}_t\\) is obtained relying only on previous observations to \\(x_t\\) (past data), then the technique is within the prediction model-based methods. Estimation: Median Absolute Deviation (MAD) Exponentially Weighted Moving Average (EWMA) method Extreme Studentized Deviate (ESD) STL decomposition Prediction: ARIMA model ARIMA model within a sliding window to compute the prediction interval, so the parameters are refitted each time that the window moves a step forward. Extreme value theory Density-based: Techniques within this group consider that points with less than \\(k\\) neighbors are outliers: using sliding windows. Histogramming 5.1.3 Types of anomalies in time series Additive outliers: For example, we are tracking users at our website and we see an uexpected growth of users in a short period of time that looks like a spike. Temporal changes: For example, when our server goes down and you see zero or a really low number of users for some short period of time. Level shifts: In the case that you deal with some conversion funnel, there could be a drop in a conversion rate. If this happens, the target metric usually doesn’t change the shape of a signal, but rather its total value for a period. 5.1.4 Methodological Approaches Statistical based methods Forecasting-based approaches: In thi methodology, a prediction is performed with a forecasting model for the next time period and if forecasted value is out of confidence interval, the sample is flagged as anomaly. Neural Network Based Approaches Clustering Based Approaches: The idea behind usage of clustering in anomaly detection is that outliers don’t belong to any cluster or has their own clusters. Proximity Based Approaches Tree Based Approaches Dimension Reduction Based Approaches 5.1.4.1 How to select the best approache Before starting the study, answer the following questions: How much data do you have retroactively? Univariate or multivariate data? What is the frequency of making anomaly detection?(near real time, hourly, weekly?) The number of anomalies is another concern. Most anomaly detection algorithms have a scoring process internally, so you are able to tune the number of anomalies by selecting an optimum threshold. Most of the time, clients dont want to be disturbed with too many anomalies even if they are real anomalies. Therefore, you might need a separate false positive elimination module. 5.2 Statistical-based approaches 5.2.1 Descriptive statistics 5.2.1.1 Histogram A basic way to detect outliers is to draw a histogram of thed ata library(ggplot2) dat &lt;- ggplot2::mpg ggplot(dat) + aes(x = hwy) + geom_histogram(bins = 30L, fill = &quot;#0c4c8a&quot;) + theme_minimal() 5.2.1.2 Boxplot: IQR (Interquartile range) A boxplot helps to visualize a quantitative variable by dsplaying 4 common location summary (min, median, first and third quartiles, max) and any observation that was classified as a suspected outlier using the interquartile range (IQR) criteria. The IQR criteria means that all obsevations above \\(q_{0.75} + 1.5 * IQR\\) or below \\(q_{0.25} - 1.5 * IQR\\) (where \\(q_{0.75}\\) and \\(q_{0.25}\\) correspond to first and third quartile respectively, and \\(IQR\\) is the difference between the third and first quartile) are considered as potential outliers.. ggplot(dat) + aes(x = &quot;&quot;, y = hwy) + geom_boxplot(fill = &quot;#0c4c8a&quot;) + theme_minimal() 5.2.1.3 Hampler filter Hampler filter consists of considering as outliers the values ourside the interval \\((I)\\) formed by the median, plus or minus 3 median absolute deviations $(MAD)*: \\[I=[median - 3 * MAD; median + 3 * MAD]\\] where \\(MAD\\) is the median obsolute deviation and is defined as the median of the absolute deviations from the data’s median \\[MAS = median(|X_{i}-med_{X}|)\\] # lower_bound lower_bound &lt;- median(dat$hwy) - 3 * mad(dat$hwy) lower_bound ## [1] 1.761 # upper_bound upper_bound &lt;- median(dat$hwy) + 3 * mad(dat$hwy) upper_bound ## [1] 46.239 # outlier outlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound) outlier_ind ## integer(0) 5.2.2 Statistical tests 5.2.2.1 Grubbs’s test https://www.statsandr.com/blog/outliers-detection-in-r/ 5.2.2.2 Dixon’s test https://www.statsandr.com/blog/outliers-detection-in-r/ 5.2.2.3 Rosner’s test https://www.statsandr.com/blog/outliers-detection-in-r/ 5.2.2.4 Z score Z-scores can quantify the usefulness of an observation when your data follow the normal distribution. Z-scores are the number of standard deviations above and below the mean that each value falls. For example, a z-score of 2 indicates that an observation is two standard deviations above the average while a z-score of -2 signifies it is two standard deviations below the mean. To calculate a z-score for an observation, take the raw measurement, substract the mean, and divide by the standard deviation. Mathematically, the formule for that process is the following: \\[ Z = \\frac{X - \\mu}{\\sigma}\\] where \\(\\mu\\) is the mean of the population and \\(\\sigma\\) is the standard deviation of the population. The further away an observation’s z-score is from zero, the more unusual it is. A standard cut-off value for finding outliers are z-scores of +/- 3 further from zero. In a population that follows the normal distribution, z-score values more extreme than +/- 3 have a probability of 0.0027 (2*0.00135), which is about 1 in 370 observations. However if your data don’t follow the normal distribution, this approach might not be accurate. 5.2.3 STL decomposition STL stands for seasonal-trend decomposition procedure based on Loess. This technique gives you an ability to split your time series signal into three parts: seasonal, trend and residue. The leading implementation of this approach is Twitter’s Anomaly Detection library. It uses Generalized Extreme Student Deviation test to check if a residual point is an outlier. 5.2.4 Generalized ESD Test for Outliers https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm 5.2.5 Extreme Studentized Deviate Technique (ESD) the Extreme Studentized Deviate (ESD) test is employed to make the decision: the null hypothesis considered is that there are no outliers, whereas the alternative is that there are up to k. Regardless of the temporal correlation, the algorithm computes k test statistics iteratively to detect k point outliers. At each iteration, it removes the most outlying observation (i.e., the furthest from the mean value). https://arxiv.org/pdf/1704.07706.pdf 5.3 Forecasting-based approaches 5.3.1 Moving Average Method In this method with the help of the moving average of past data, present-day value is estimated. A moving average can be exponential Moving average or simple Moving aberage. The expoential moving average gives more weight to recent data. 5.3.2 ARMA 5.3.3 Prophet Prophet” was Published by Facebook which uses additive regression model. This model helps in detecting anomalies. Prophet automatically detects changes in trends by selecting change points from the data and also do some modification in seasonal components (year, month) by some techniques like Fourier Transform. 5.4 Neural Network Based Approaches 5.4.1 Autoencoder Autoencoder is an unsupervised type neural networks, and mainly used for feature extraction and dimension reduction. At the same time, it is a good option for anomaly detection problems. Autoencoder consists of encoding and decoding parts. In encoding part, main features are extracted which represents the patterns in the data, and then each samples is reconstructed in the decoding part. The reconstruction error will be minumum for normal samples. On the other hand, the model is not able to reconstruct a sample that behaves abnormal, resulting a high reconstruction error. So, basically, the higher reconstruction error a sample has, the more likely it is to be an anomaly. 5.5 Clustering Based Approaches 5.5.1 Kmeans 5.5.2 Gaussina Mixture Model (GMM) It attemps to find a mixture of a finite number of Gaussian distributions inside the dataset. 5.5.3 DBSCAN DBSCAN is a density based clustering algorithm. It determines the core points in the dataset which contains at least min_samples around it within epsilon distance, and creates clusters from these samples. After that, it finds all points which are densely reachable(within epsilon distance) from any sample in the cluster and add them to the cluster. And then, iteratively, it performs the same procedure for the newly added samples and extend the cluster. DBSCAN determines the cluster number by itself, and outliers samples will be assigned as -1. In other words, it directly serves for anomaly detection. Note that, it might suffer from perfromance issues with large sized datasets. 5.6 Proximity Based Approaches 5.6.1 K-Nearest neighbor: The first algorithm that come to mind is k-nearest neighbor(k-NN) algorithm. The simple logic behind is that outliers are far away from the rest of samples in the data plane. The distances to nearest negihbors of all samples are estimated and the samples located far from the other samples can be flagged as outlier. k-NN can use different distance metrics like Eucledian, Manhattan, Minkowski, Hamming distance etc. 5.6.2 Local Outlier Factor (LOF) It identifies the local outliers with respect to local neighbors rather than global data distribution. It utilizes a metric named as local reachability density(lrd) in order to represents density level of each points. LOF of a sample is simply the ratio of average lrd values of the sample’s neighbours to lrd value of the sample itself. If the density of a point is much smaller than average density of its neighbors, then it is likely to be an anomaly. 5.7 Tree Based Approaches 5.7.1 Isolation Forest Isolation Forest is a tree based, very effective algorithm for detecting anomalies. It builds multiple trees. To build a tree, it randomly picks a feature and a split value within the minimums and maximums values of the corresponding feature. This procedure is applied to all samples in the dataset. And finally, a tree ensemble is composed by averaging all trees in the forest. The idea behind the Isolation Forest is that outliers are easy to diverge from rest of the samples in dataset. For this reason, we expect shorter paths from root to a leaf node in a tree(the number of splittings required to isolate the sample) for abnormal samples compared to rest of the samples in dataset. 5.8 Dimension Reduction Based Approaches 5.8.1 Principal Component Analyses (PCA) Principal Component Analyses (PCA) is mainly used as a dimension reduction method for high dimensional data. In a basic manner, it helps to cover most of the variance in data with a smaller dimension by extracting eigenvectors that have largest eigenvalues. Therefore, it is able to keep most of the information in the data with a very smaller dimension. While using PCA in anomaly detection, it follows a very similar approach like Autoencoders. Firstly, it decomposes data into a smaller dimension and then it reconstructs data from the decomposed version of data again. Abnormal samples tend to have a high reconstruction error regarding that they have different behaviors from other observations in data, so it is diffucult to obtain same observation from the decomposed version. PCA can be a good option for multivariate anomaly detection scenarios. 5.9 References https://medium.com/learningdatascience/anomaly-detection-techniques-in-python-50f650c75aaf A review on outlier/anomaly detection in time series data: https://arxiv.org/pdf/2002.04236.pdf "],
["references-3.html", "References", " References "]
]
